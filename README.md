# LLM Multi-Class Text Classification Using DistilBERT

Objective: To classify text into multiple predefined categories using a lightweight transformer model.

DistilBERT is a distilled version of BERT, which means it's smaller, faster, and less resource-intensive while maintaining much of the original BERT's performance. It's ideal for applications where computational resources are limited or speed is critical.

Used Hugging Face's transformers library for implementing DistilBERT, scikit-learn for preprocessing and PyTorch for model training.
